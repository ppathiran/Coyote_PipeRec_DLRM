{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The binary dataset has the size of 8801398464\n",
      "The binary dataset contains 45840617 rows.\n",
      "file_id:  1\n",
      "file_id:  2\n",
      "file_id:  3\n",
      "file_id:  4\n",
      "file_id:  5\n",
      "file_id:  6\n",
      "file_id:  7\n",
      "file_id:  8\n",
      "Total execution time:  104.91418614100257\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "datfile = '/home/yuzhuyu/u55c/Network_CPU/CPP/processed_data.bin'\n",
    "# Assume we want to read rows of data with 48 32-bit little endian integers\n",
    "file_size = path.getsize(datfile)\n",
    "print(f\"The binary dataset has the size of {file_size}\")\n",
    "num_integers = 48\n",
    "integer_size = 4  # Size of one integer in bytes (4 bytes for 32-bit integer)\n",
    "row_size = num_integers * integer_size  # Total size of a row in bytes\n",
    "total_count = file_size // row_size\n",
    "print(f\"The binary dataset contains {total_count} rows.\")\n",
    "\n",
    "days = 8\n",
    "total_per_file = []\n",
    "total_per_file.append(total_count)\n",
    "# reset total per file due to split\n",
    "num_data_per_split, extras = divmod(total_count, days)\n",
    "total_per_file = [num_data_per_split] * days\n",
    "for j in range(extras):\n",
    "    total_per_file[j] += 1\n",
    "# split into days (simplifies code later on)\n",
    "# file_id = 0\n",
    "\n",
    "\n",
    "sif_output = [[] for _ in range(days)]\n",
    "# current_file_data = sif_output[file_id]\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "if path.exists(datfile):\n",
    "    with open(datfile, 'rb') as f:  # Open in binary mode\n",
    "        # while True:\n",
    "        file_id = 0\n",
    "        while file_id < days: \n",
    "\n",
    "            boundary = total_per_file[file_id]\n",
    "            read_size = boundary * row_size\n",
    "            integer_size = boundary * num_integers\n",
    "            current_file_data = sif_output[file_id]\n",
    "            \n",
    "            binary_row = f.read(read_size)  # Read one row of binary data\n",
    "            if binary_row:  # If there's data to read\n",
    "\n",
    "                format_string = f'<{integer_size}I'  # Little-endian, 32-bit unsigned integers\n",
    "                line = struct.unpack(format_string, binary_row)\n",
    "                \n",
    "                # if j == boundary:\n",
    "                \n",
    "                # MODIFIED: Switch to the next inner list for the new \"file\"or split of data.\n",
    "                \n",
    "                current_file_data.append(line)\n",
    "                # MODIFIED: Append line to the in-memory data structurinstead of writing to a file.\n",
    "                file_id += 1\n",
    "                # j += 1\n",
    "                # print(integers)\n",
    "            else:\n",
    "                print(\"No data to read or file is empty\")\n",
    "                break\n",
    "            print(\"file_id: \", file_id)\n",
    "else:\n",
    "    print(\"No data file\")\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "print(\"Total execution time: \", (t1-t0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python program to explain os.read() method  \n",
    "    \n",
    "# importing os module  \n",
    "import os \n",
    "\n",
    "path = \"/home/yuzhuyu/u55c/Network_CPU/CPP/processed_data_512.bin\"\n",
    "fd = os.open(path, os.O_RDONLY) \n",
    "n = 50\n",
    "readBytes = os.read(fd, n) \n",
    "  \n",
    "# Print the bytes read \n",
    "print(readBytes) \n",
    "  \n",
    "# close the file descriptor \n",
    "os.close(fd) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys\n",
    "from multiprocessing import Manager, Process\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "# import io\n",
    "# from io import StringIO\n",
    "# import collections as coll\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def getCriteoAdData(\n",
    "    datafile,\n",
    "    o_filename,\n",
    "    max_ind_range=-1,\n",
    "    sub_sample_rate=0.0,\n",
    "    days=7,\n",
    "    data_split=\"train\",\n",
    "    randomize=\"total\",\n",
    "    criteo_kaggle=True,\n",
    "    memory_map=False,\n",
    "    dataset_multiprocessing=False,\n",
    "):\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # split the datafile into path and filename\n",
    "    lstr = datafile.split(\"/\")\n",
    "    d_path = \"/\".join(lstr[0:-1]) + \"/\"\n",
    "    d_file = lstr[-1].split(\".\")[0] if criteo_kaggle else lstr[-1]\n",
    "    npzfile = d_path + ((d_file + \"_day\") if criteo_kaggle else d_file)\n",
    "    trafile = d_path + ((d_file + \"_fea\") if criteo_kaggle else \"fea\")\n",
    "\n",
    "    # count number of datapoints in training set\n",
    "    total_file = d_path + d_file + \"_day_count.npz\"\n",
    "\n",
    "    total_count = 0\n",
    "    total_per_file = []\n",
    "    if criteo_kaggle:\n",
    "        if path.exists(datafile):\n",
    "            t0_0 = time.perf_counter()\n",
    "            print(\"Reading data from path=%s\" % (datafile))\n",
    "            with open(str(datafile)) as f:\n",
    "                for _ in f:\n",
    "                    total_count += 1\n",
    "            total_per_file.append(total_count)\n",
    "            # reset total per file due to split\n",
    "            num_data_per_split, extras = divmod(total_count, days)\n",
    "            total_per_file = [num_data_per_split] * days\n",
    "            for j in range(extras):\n",
    "                total_per_file[j] += 1\n",
    "            # split into days (simplifies code later on)\n",
    "            file_id = 0\n",
    "            boundary = total_per_file[file_id]\n",
    "            t0_1 = time.perf_counter()\n",
    "\n",
    "            print(\"Counting row number: %s s\", (t0_1-t0_0))\n",
    "\n",
    "            FILE_SIZE = 11147184845\n",
    "            BLOCK_SIZE = FILE_SIZE\n",
    "            buffer_count = 0\n",
    "            fd = os.open(datafile, os.O_RDONLY | os.O_DIRECT)\n",
    "            try:\n",
    "                while True:\n",
    "                    try:\n",
    "                        buffer = os.read(fd, BLOCK_SIZE)\n",
    "                        if not buffer:  # If the read returns an empty buffer, EOF is reached\n",
    "                            break\n",
    "                        buffer_count += buffer.count(b'\\n')\n",
    "                    except OSError as e:\n",
    "                        if e.errno == os.errno.EINVAL:\n",
    "                            # Handle the case where buffer alignment is causing EINVAL\n",
    "                            pass\n",
    "                        \n",
    "            finally:\n",
    "                os.close(fd)\n",
    "\n",
    "            t0_2 = time.perf_counter()\n",
    "\n",
    "            \n",
    "            print(\"Reading files: %s s\", (t0_2-t0_1))\n",
    "            print(\"Reading throughput: \", (FILE_SIZE / (t0_2-t0_1)))\n",
    "            print(\"Total bytes: \", buffer_count)\n",
    "\n",
    "            # #! MODIFIED: Instead of opening new files, initialize a list to store all file data in memory.\n",
    "            # sif_output = [[] for _ in range(days)]\n",
    "            # current_file_data = sif_output[file_id]\n",
    "            # with open(str(datafile)) as f:\n",
    "            #     for j, line in enumerate(f):\n",
    "            #         if j == boundary:\n",
    "            #             file_id += 1\n",
    "            #             # MODIFIED: Switch to the next inner list for the new \"file\" or split of data.\n",
    "            #             current_file_data = sif_output[file_id]\n",
    "            #             boundary += total_per_file[file_id]\n",
    "            #         # MODIFIED: Append line to the in-memory data structure instead of writing to a file.\n",
    "            #         current_file_data.append(line)\n",
    "            # #! MODIFICATION FINISHES\n",
    "        else:\n",
    "            sys.exit(\n",
    "                \"ERROR: Criteo Kaggle Display Ad Challenge Dataset path is invalid; please download from https://labs.criteocom/2014/02/kaggle-display-advertising-challenge-dataset\"\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    \n",
    "    print(\"Splitting Input Files: %s s\", (t1-t0))\n",
    "    # print(\"Total Execution Time: %s s\", (t5-t0))\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def loadDataset(\n",
    "    dataset,\n",
    "    max_ind_range,\n",
    "    sub_sample_rate,\n",
    "    randomize,\n",
    "    data_split,\n",
    "    raw_path=\"\",\n",
    "    pro_data=\"\",\n",
    "    memory_map=False,\n",
    "    dataset_multiprocessing=False\n",
    "):\n",
    "    # dataset\n",
    "    if dataset == \"kaggle\":\n",
    "        days = 8\n",
    "        o_filename = \"kaggleAdDisplayChallenge_processed\"\n",
    "    elif dataset == \"terabyte\":\n",
    "        days = 24\n",
    "        o_filename = \"terabyte_processed\"\n",
    "    else:\n",
    "        raise (ValueError(\"Data set option is not supported\"))\n",
    "\n",
    "    # split the datafile into path and filename\n",
    "    lstr = raw_path.split(\"/\")\n",
    "    d_path = \"/\".join(lstr[0:-1]) + \"/\"\n",
    "    d_file = lstr[-1].split(\".\")[0] if dataset == \"kaggle\" else lstr[-1]\n",
    "    npzfile = (d_file + \"_day\") if dataset == \"kaggle\" else d_file\n",
    "    # trafile = d_path + ((d_file + \"_fea\") if dataset == \"kaggle\" else \"fea\")\n",
    "\n",
    "    # check if pre-processed data is available\n",
    "    data_ready = True\n",
    "    if memory_map:\n",
    "        for i in range(days):\n",
    "            reo_data = d_path + npzfile + \"_{0}_reordered.npz\".format(i)\n",
    "            if not path.exists(str(reo_data)):\n",
    "                data_ready = False\n",
    "    else:\n",
    "        if not path.exists(str(pro_data)):\n",
    "            data_ready = False\n",
    "\n",
    "    # pre-process data if needed\n",
    "    # WARNNING: when memory mapping is used we get a collection of files\n",
    "    if data_ready:\n",
    "        print(\"Reading pre-processed data=%s\" % (str(pro_data)))\n",
    "        file = str(pro_data)\n",
    "    else:\n",
    "        print(\"Reading raw data=%s\" % (str(raw_path)))\n",
    "        file = getCriteoAdData(\n",
    "            raw_path,\n",
    "            o_filename,\n",
    "            max_ind_range,\n",
    "            sub_sample_rate,\n",
    "            days,\n",
    "            data_split,\n",
    "            randomize,\n",
    "            dataset == \"kaggle\",\n",
    "            memory_map,\n",
    "            dataset_multiprocessing\n",
    "        )\n",
    "\n",
    "    return file, days\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ### import packages ###\n",
    "    import argparse\n",
    "\n",
    "    ### parse arguments ###\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocess Criteo dataset\")\n",
    "    # model related parameters\n",
    "    parser.add_argument(\"--max-ind-range\", type=int, default=-1)\n",
    "    parser.add_argument(\"--data-sub-sample-rate\", type=float, default=0.0)  # in [0, 1]\n",
    "    parser.add_argument(\"--data-randomize\", type=str, default=\"total\")  # or day or none\n",
    "    parser.add_argument(\"--memory-map\", action=\"store_true\", default=False)\n",
    "    parser.add_argument(\"--data-set\", type=str, default=\"kaggle\")  # or terabyte\n",
    "    parser.add_argument(\"--raw-data-file\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--processed-data-file\", type=str, default=\"\")\n",
    "    parser.add_argument(\"--dataset-multiprocessing\", action=\"store_true\", default=False)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    loadDataset(\n",
    "        args.data_set,\n",
    "        args.max_ind_range,\n",
    "        args.data_sub_sample_rate,\n",
    "        args.data_randomize,\n",
    "        \"train\",\n",
    "        args.raw_data_file,\n",
    "        args.processed_data_file,\n",
    "        args.memory_map,\n",
    "        args.dataset_multiprocessing\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
