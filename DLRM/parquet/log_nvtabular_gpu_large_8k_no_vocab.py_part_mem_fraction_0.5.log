python nvtabular_gpu_large_8k_no_vocab.py --part_mem_fraction 0.1
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (2650800296) (bytes) of parquet file is bigger than requested part_size (2576980377) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 12.35 seconds
Fitting the workflow
Workflow fitting completed in 4.09 seconds
Transforming the dataset
Dataset transformation completed in 0.43 seconds
Total execution time: 16.88 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (2650800296) (bytes) of parquet file is bigger than requested part_size (2576980377) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.90 seconds
Fitting the workflow
Workflow fitting completed in 4.23 seconds
Transforming the dataset
Dataset transformation completed in 0.43 seconds
Total execution time: 8.56 seconds


python nvtabular_gpu_large_8k_no_vocab.py --part_mem_fraction 0.2
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.75 seconds
Fitting the workflow
Workflow fitting completed in 4.20 seconds
Transforming the dataset
Dataset transformation completed in 0.44 seconds
Total execution time: 8.39 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.93 seconds
Fitting the workflow
Workflow fitting completed in 4.17 seconds
Transforming the dataset
Dataset transformation completed in 0.45 seconds
Total execution time: 8.55 seconds


python nvtabular_gpu_large_8k_no_vocab.py --part_mem_fraction 0.3
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.78 seconds
Fitting the workflow
Workflow fitting completed in 32.01 seconds
Transforming the dataset
Dataset transformation completed in 0.43 seconds
Total execution time: 36.22 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.88 seconds
Fitting the workflow
Workflow fitting completed in 4.95 seconds
Transforming the dataset
Dataset transformation completed in 0.43 seconds
Total execution time: 9.26 seconds


python nvtabular_gpu_large_8k_no_vocab.py --part_mem_fraction 0.4
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.82 seconds
Fitting the workflow
Workflow fitting completed in 31.30 seconds
Transforming the dataset
Dataset transformation completed in 0.47 seconds
Total execution time: 35.58 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.78 seconds
Fitting the workflow
Workflow fitting completed in 5.95 seconds
Transforming the dataset
Dataset transformation completed in 0.42 seconds
Total execution time: 10.15 seconds


python nvtabular_gpu_large_8k_no_vocab.py --part_mem_fraction 0.5
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.89 seconds
Fitting the workflow
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 112, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 577, in read_parquet
    df = _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 721, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 829, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 118, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 174, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 101, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 91, in main
    transformed_data = preprocess_criteo_parquet(train_file, frequency_threshold, args.part_mem_fraction)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 70, in preprocess_criteo_parquet
    workflow.fit(train_ds)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtabular/workflow/workflow.py", line 213, in fit
    self.executor.fit(dataset, self.graph)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 479, in fit
    Dataset(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py", line 1262, in sample_dtypes
    _real_meta = self.engine.sample_data(n=n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset_engine.py", line 71, in sample_data
    _head = _ddf.partitions[partition_index].head(n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1590, in head
    return self._head(n=n, npartitions=npartitions, compute=compute, safe=safe)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1624, in _head
    result = result.compute()
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 379, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 97, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 645, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 646, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 326, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Workflow creation completed in 0.00 seconds
Dataset loading completed in 3.82 seconds
Fitting the workflow
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 112, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 577, in read_parquet
    df = _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 721, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 829, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 118, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 174, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 101, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 91, in main
    transformed_data = preprocess_criteo_parquet(train_file, frequency_threshold, args.part_mem_fraction)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_8k_no_vocab.py", line 70, in preprocess_criteo_parquet
    workflow.fit(train_ds)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtabular/workflow/workflow.py", line 213, in fit
    self.executor.fit(dataset, self.graph)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 479, in fit
    Dataset(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py", line 1262, in sample_dtypes
    _real_meta = self.engine.sample_data(n=n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset_engine.py", line 71, in sample_data
    _head = _ddf.partitions[partition_index].head(n)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1590, in head
    return self._head(n=n, npartitions=npartitions, compute=compute, safe=safe)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/core.py", line 1624, in _head
    result = result.compute()
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 379, in compute
    (result,) = compute(self, traverse=False, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 97, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 645, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 646, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 326, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp


