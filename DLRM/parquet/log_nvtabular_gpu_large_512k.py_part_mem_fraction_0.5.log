python nvtabular_gpu_large_512k.py --part_mem_fraction 0.1
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (2650800296) (bytes) of parquet file is bigger than requested part_size (2576980377) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 22.69 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 27.00 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py:343: UserWarning: Row group memory size (2650800296) (bytes) of parquet file is bigger than requested part_size (2576980377) for the NVTabular dataset.A row group memory size of 128 MB is generally recommended. You can find info on how to set the row group size of parquet files in https://nvidia-merlin.github.io/NVTabular/main/resources/troubleshooting.html#setting-the-row-group-size-for-the-parquet-files
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 22.21 seconds
Transforming the dataset
Dataset transformation completed in 0.35 seconds
Total execution time: 26.41 seconds


python nvtabular_gpu_large_512k.py --part_mem_fraction 0.2
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
Fitting the workflow
Workflow fitting completed in 22.35 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 26.64 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
Fitting the workflow
Workflow fitting completed in 21.57 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 25.79 seconds


python nvtabular_gpu_large_512k.py --part_mem_fraction 0.3
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 23.30 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 27.43 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 22.93 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 27.11 seconds


python nvtabular_gpu_large_512k.py --part_mem_fraction 0.4
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 22.50 seconds
Transforming the dataset
Dataset transformation completed in 0.35 seconds
Total execution time: 26.71 seconds
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Workflow fitting completed in 22.49 seconds
Transforming the dataset
Dataset transformation completed in 0.34 seconds
Total execution time: 26.69 seconds


python nvtabular_gpu_large_512k.py --part_mem_fraction 0.5
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 112, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 577, in read_parquet
    df = _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 721, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 829, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 118, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 174, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 95, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 85, in main
    transformed_data = preprocess_criteo_parquet(train_file, frequency_threshold, args.part_mem_fraction)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 64, in preprocess_criteo_parquet
    workflow.fit(train_ds)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtabular/workflow/workflow.py", line 213, in fit
    self.executor.fit(dataset, self.graph)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 466, in fit
    self.fit_phase(dataset, current_phase)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 541, in fit_phase
    results = dask.compute(stats, scheduler="synchronous")[0]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 97, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 645, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 646, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 326, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'
  warn(f"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'
  warn(f"Triton dtype mappings did not load successfully due to an error: {exc.msg}")
/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/dataset.py:301: UserWarning: Using very large partitions sizes for Dask. Memory-related errors are likely.
  warnings.warn(
Fitting the workflow
Traceback (most recent call last):
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 301, in read_partition
    cls._read_paths(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 112, in _read_paths
    df = cudf.read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 577, in read_parquet
    df = _parquet_to_frame(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 721, in _parquet_to_frame
    return _read_parquet(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtx/nvtx.py", line 116, in inner
    result = func(*args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/cudf/io/parquet.py", line 829, in _read_parquet
    return libparquet.read_parquet(
  File "parquet.pyx", line 118, in cudf._lib.parquet.read_parquet
  File "parquet.pyx", line 174, in cudf._lib.parquet.read_parquet
MemoryError: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 95, in <module>
    main()
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 85, in main
    transformed_data = preprocess_criteo_parquet(train_file, frequency_threshold, args.part_mem_fraction)
  File "/home/yuzhuyu/u55c/rec_preprocessing/nvtabular_gpu_large_512k.py", line 64, in preprocess_criteo_parquet
    workflow.fit(train_ds)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/nvtabular/workflow/workflow.py", line 213, in fit
    self.executor.fit(dataset, self.graph)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 466, in fit
    self.fit_phase(dataset, current_phase)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/dag/executors.py", line 541, in fit_phase
    results = dask.compute(stats, scheduler="synchronous")[0]
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/base.py", line 665, in compute
    results = schedule(dsk, keys, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 97, in __call__
    return read_parquet_part(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 645, in read_parquet_part
    dfs = [
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask/dataframe/io/parquet/core.py", line 646, in <listcomp>
    func(
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/merlin/io/parquet.py", line 127, in read_partition
    return CudfEngine.read_partition(fs, pieces, *args, **kwargs)
  File "/home/yuzhuyu/miniconda3/envs/nvtabular/lib/python3.9/site-packages/dask_cudf/io/parquet.py", line 326, in read_partition
    raise MemoryError(
MemoryError: Parquet data was larger than the available GPU memory!

See the notes on split_row_groups in the read_parquet documentation.

Original Error: std::bad_alloc: out_of_memory: CUDA error at: /home/yuzhuyu/miniconda3/envs/nvtabular/include/rmm/mr/device/cuda_memory_resource.hpp


